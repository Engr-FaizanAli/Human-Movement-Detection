Motion-First Pipeline for Long-Range Thermal Object Detection
Challenge: Distant Targets Appear as "Blobs" in Thermal Imagery

Thermal imaging at range comes with unique challenges. Your YOLO-based model performs well on typical scenarios where humans or cars are reasonably large and distinct in the frame. For example, a nearby human stands out clearly and is detected with high confidence by the model 
. However, at very long distances (e.g. 6–7 km), targets like humans may occupy only a few pixels. In such cases they appear as amorphous moving blobs nearly indistinguishable from background clutter by appearance alone
. Thermal cameras have inherent limitations at these ranges: low resolution and contrast, sensor noise, and lack of detailed shape cues. A distant person might blend into moving foliage or hot ground, making it difficult for a CNN detector to recognize them purely from a single frame. The only reliable cue in this scenario is motion – these blobs move in a coherent way that background elements (or noise) do not. This calls for a pipeline that prioritizes motion detection to spot any moving object first, then uses the trained model to classify those candidates.

Stage 1: Motion Detection for Candidate Regions of Interest (ROIs)

The first stage of the pipeline should be a motion detection module that flags regions where something is moving, without needing to know what the object is. This stage acts as a class-agnostic trigger, ensuring that even unrecognizable distant targets are not missed due to lack of training data.

Background Subtraction: If your camera is fixed (or mostly fixed), background subtraction is a proven approach to detect moving blobs. In video surveillance, motion detection methods generally fall into three categories: simple frame differencing, optical flow, and background modeling. Among these, background subtraction is especially popular for its efficiency and accuracy in practice. The idea is to build a model of the static background and detect deviations (foreground) when an object moves. Modern background subtractors (e.g. Mixture-of-Gaussians models like OpenCV’s MOG2, or the ViBe algorithm) can adapt to gradual changes in the scene while highlighting new moving objects. For instance, the ViBe algorithm has been shown to “quickly, accurately and integrally detect [a] moving target” in various scenes by modeling each pixel’s background distribution. An improved version of ViBe even uses a “time of map” mechanism to reduce false alarms (ghosting) from things like an initially present moving object or long-static objects. In practice, you would initialize a background model over several frames and continuously update it as lighting or environment changes, while masking out any regions that differ significantly from the background as motion.

Frame Differencing / Optical Flow: In cases where the camera is rotating or panning (e.g. a PTZ camera scanning the area), a full background model may be harder to maintain. A simpler approach is consecutive frame differencing – subtract one frame from the next to see what’s changed. This can work if the camera movement is slow or if you can stabilize the video in software. You might take a short window of frames (for example, compare the current frame with one from a second ago) to account for slow-moving objects. For more robust detection of motion (especially if the camera itself moves or vibrates), optical flow methods (like Farneback or Lucas-Kanade) can detect motion vectors across the image. Optical flow will tell you the magnitude and direction of motion for each region, which can help differentiate a moving person from swaying trees (the motion of foliage tends to be high-frequency, jittery, and confined to the leaf edges, whereas a walking human produces a more coherent translational motion). Optical flow is computationally heavier than background subtraction, but it can be useful if background modeling fails.

Filtering and Refinement: Whichever motion detection approach you use, the raw foreground mask will likely need cleanup. Thermal noise or moving vegetation can produce false-positive blobs. Use techniques like morphological filtering (erosion/dilation) to remove speckle noise and close small gaps in the detected regions. You can also apply a size filter to ignore tiny specs that are too small to be a person or vehicle at that range (e.g. filter out anything that is only 1–2 pixels). Another trick is to incorporate a temporal consistency check: only declare a motion ROI if it persists across a few consecutive frames in roughly the same trajectory. This helps ignore one-off flashes or momentary flickers. In research literature, more advanced statistical methods exist (e.g. adaptive contour-based models or random field models for thermal imagery) that improve robustness, but even a well-tuned classic approach can be very effective in practice. The key is to ensure this stage is sensitive enough to catch small moving targets, while rejecting repetitive background motion. By the end of Stage 1, the pipeline should output candidate ROIs (bounding boxes or blobs) wherever motion was detected. These ROIs indicate “something moved here” but not what it is yet.

Stage 2: YOLO-Based Object Recognition on Motion ROIs

Once a moving region is detected, the next stage is to identify what that object is (human, car, or perhaps false alarm). Here you leverage your fine-tuned YOLO model (or a similar object detector) on the candidate regions or frames:

Focused Detection on ROIs: You can crop the frame to each ROI or simply feed the whole frame to YOLO but pay attention only to detections overlapping the motion regions. The motion mask acts like an attention mechanism – it narrows down where YOLO should find objects. In many cases, this will substantially reduce false positives, because the model won’t be distracted by static objects that might superficially resemble a human (e.g. a warm rock or an oddly shaped bush) if they aren’t moving. Given your model’s performance (Precision ~0.90, Recall ~0.79 overall), it already does well on clear thermal targets. Running it after motion filtering means YOLO only needs to confirm true positives on genuine moving objects, which should further boost precision.

Leverage YOLO’s Strengths: YOLO is a single-stage detector known for real-time speed and decent accuracy. It processes an image in one pass and outputs bounding boxes and class probabilities for objects it recognizes. This makes it suitable for frame-by-frame analysis in video. By fine-tuning YOLO on thermal imagery (as you did with ~30k TI frames), it has learned to detect humans and cars under typical thermal appearances. For moderately sized objects (say a human at a few hundred meters or a car at a km), YOLO will confidently label them, as seen in your results (human AP ~87% at IoU 0.5). For example, in the image below a person at mid-range is detected as 'Human' with high confidence by the model
. The model picks up the distinct human shape and heat signature clearly in this scenario.

Dealing with Very Small Blobs: The tricky part is when the ROI is just a few pixels (the 6–7 km case). Off-the-shelf YOLO might not have learned to classify such tiny, featureless blobs as humans, because those were likely not well-represented in your training data. You have two options (which are not mutually exclusive): improve the model for small objects and/or treat such cases differently. To improve the model, you could incorporate a small-object training strategy – e.g., include some far-away blob examples (if available) in training, use higher-resolution input or tiling (cropping the image into patches so that small targets appear larger in the input), or adjust YOLO’s anchor sizes and network layers to be more sensitive to small scales. This can marginally help the detector recognize tiny objects. However, an alternate approach is to acknowledge that extremely small blobs might never have enough visual features for confident classification, and instead flag them simply as “unknown moving object” unless/until they come closer. Since your goal is likely surveillance (to know something is there at all), detecting the presence of a moving blob is still a win. The motion detection stage already did that; YOLO can attempt a classification, but if it’s below confidence threshold, you might still output an alert that “motion was detected at long range” even if the class is uncertain. In other words, don’t depend entirely on the learned model for far targets – use the model when possible, but fall back to the knowledge that “something is moving” when the model isn’t sure. This reduces reliance on having exhaustive training data for every distance.

Hierarchical Decision: A practical pipeline might implement a two-tier confidence check: if YOLO returns a detection with reasonable confidence (e.g. above a threshold) on the motion ROI, then you confirm it as that class (human/car). This covers the cases where the object is close or clear enough. If YOLO either fails to detect anything or gives very low confidence for the ROI (which could happen for those tiny blobs), you don’t discard the ROI outright. Instead, you can either classify it as unknown or at least pass it along to tracking (Stage 3) as an unclassified moving target. In summary, Stage 2 adds semantic knowledge to the motion regions, using the power of your trained model for recognizable objects but not ignoring unclassified movers.

Stage 3: Tracking and Multi-Frame Integration

After detection and initial classification, incorporating a tracking mechanism will greatly enhance the system’s robustness – especially for those small, far-away objects that flicker in and out of detection. Tracking ensures that once an object is detected moving, you can follow it over time, improving confidence in its presence and helping deal with temporary detection dropouts.

Why Tracking Matters: Even the best detectors can miss an object in some frames (due to motion blur, thermal clutter, or the object partially occluding behind an obstacle). By using a tracking algorithm, you can maintain the object’s identity and position between detection hits. For example, in the thermal scene below, two people are being tracked and have persistent IDs (#1 and #4) despite partial occlusion by trees
. The tracker links their detections over time. If one frame were to miss a detection (say person #4 is momentarily behind a bush and YOLO doesn’t see them), a tracker can predict that location and pick them back up when they reappear, instead of treating it as a brand new detection. This temporal continuity reduces false negatives and prevents false alarms like thinking it’s a new intruder each time the same person re-appears.

Multiple Object Tracking (MOT) Algorithms: Given that you might have several moving objects (cars, people) in view, a multi-object tracking algorithm is appropriate. One popular solution is DeepSORT, which stands for Deep Simple Online and Realtime Tracking. DeepSORT takes detections (e.g. from YOLO every frame) and performs a combination of Kalman filter motion prediction and appearance embedding to link detections across frames. DeepSORT is capable of spawning new tracks when a new object is detected and maintaining existing tracks. This means if an object starts moving (motion detection triggers a new ROI and YOLO identifies it), DeepSORT will create a track for it. On subsequent frames, even if the object’s detection confidence fluctuates, the tracker’s motion model can keep predicting its position and associate it with new detections. DeepSORT uses a lightweight CNN to get an “appearance feature” of each detection as well, which helps distinguish objects if multiple are present (so it doesn’t confuse two crossing people, for example). The end result is each moving object gets a unique ID and a trajectory.

ByteTrack and Modern Trackers: In recent years, even more robust trackers have emerged. For instance, ByteTrack (introduced in 2021) has gained popularity for multi-object tracking in scenarios with many missed detections. ByteTrack’s strategy is to cleverly incorporate low-confidence detections rather than discard them – it matches high-confidence detections to tracks first, then uses any unmatched low-confidence detections to keep tracks going. This is particularly useful for small blob detection, where your YOLO might output a detection but with low confidence (because it’s not 100% sure it’s a person). A traditional tracker might ignore that detection if it’s below threshold, causing the track to be lost. ByteTrack, however, can use it to continue the track, assuming it fits the predicted motion. The latest YOLOv8 framework even comes with integrated trackers like ByteTrack and Bot-SORT that can be turned on to handle tracking automatically. If you prefer not to delve into training appearance models, SORT (without the deep part) is a simpler alternative that uses just the Kalman filter and bounding box overlap (IoU) for associating detections. SORT can work well when objects are well-separated and appearance differences aren’t needed. In thermal videos with people and cars, the objects are usually distinct enough in space that SORT/DeepSORT will be effective.

Track-Before-Detect for Extreme Cases: You hinted at the idea of not depending solely on training data for those far blobs – this is exactly where a concept called “track-before-detect” can help. Normally, tracking algorithms rely on detections each frame, but track-before-detect (TBD) methods accumulate evidence of a target over multiple frames even if individual frame detections are weak. In research, TBD is used in radar and IR systems to spot very dim targets. For example, one study proposed a multi-target TBD approach using background subtraction to get a “foreground probability map” each frame, then a specialized filter (GLMB filter based on random finite set theory) to track multiple targets through that probability data. The result was improved detection of multiple thermal targets and continuous tracking even when single-frame detections were unreliable. The takeaway here isn’t that you need to implement a complex RFS filter, but rather the principle of accumulating motion evidence over time. In practical terms, this could mean: if a blob is too small to confidently detect as human in one frame, tracking its consistent movement over 5–10 frames can confirm it’s a real moving object (and not just noise), effectively detecting it through its trajectory. Your pipeline can mimic this by, say, requiring a motion ROI to persist over a few frames before raising an alarm, or by integrating the tracker’s output (which embodies multi-frame evidence) as an input to decision making. If the tracker says “an object has moved from point A to B over the last second,” you can be confident it’s an actual target even if classification was uncertain.

Additional Considerations for a Robust System

Environmental factors and tuning: Long-range thermal surveillance is sensitive to environment changes. If the camera is fixed, day/night temperature differences, rain, or fog can affect the thermal image and the background model. Make sure to periodically update the background model (e.g. have a learning rate or adapt when no motion is present) so you don’t get a stale background that triggers false motion. If the camera is on a pan-tilt mount that scans, consider segmenting the scene into zones and maintaining a separate background for each, or allow the system a “re-settle” time after a large camera move before trusting motion detection. Wind-blown vegetation is a notorious source of false motion – one way to handle this is to mask out regions that always move (you might know where trees or bushes are in the frame and ignore those regions for motion, or use longer-term averaging to identify “steady waving” areas). Another approach is to use frequency filtering: motion from living objects tends to have a trajectory and consistent velocity, whereas foliage has a high-frequency oscillation. Some advanced systems use spatio-temporal filtering to cut out motion that doesn’t “travel”; simpler, you could require that a blob moves more than a certain distance or in a coherent direction over a few frames to count as a real target.

Alert logic: Decide how you want to handle the small blob detections from a user perspective. If the ultimate goal is to alert an operator or to cue a higher-resolution sensor, you might treat any confirmed moving blob as an alarm (possibly with a label “possible human at long range”). If you have a PTZ camera, you could even cue it to zoom in on the motion region once detected, effectively getting a closer look to allow YOLO to classify it in a later stage. If the system is fully automated (no human in the loop), you might incorporate a state where an unclassified moving object at long range triggers a different behavior (e.g., tracking it until it gets closer or leaves).

Continuous Learning: Over time, collect the cases where the system had trouble (e.g., it flagged motion but YOLO couldn’t classify it). These could be opportunities to improve your model. Perhaps you’ll gather some long-range blob footage and can fine-tune a small “blob classifier” that looks at the motion ROI and tries to distinguish human vs animal vs tree movement by subtle cues. This could be a lightweight CNN that works on the cropped ROI sequence. It’s not necessary at first, but it’s a path for refining the pipeline once the basics are in place.

Recommended Pipeline Summary

Bringing it all together, here’s an outline of the proposed pipeline:

Motion Detection Stage: Use background subtraction or frame differencing to detect any moving regions in the thermal video. This yields candidate ROIs for potential objects. Emphasize sensitivity to ensure far-away movement is caught (tune thresholds, use temporal filtering to reduce noise). The output is a set of bounding boxes or masks where movement occurred in the frame.

Candidate ROI Filtering: (Optional fine-tuning) Clean up the motion mask by removing obvious false alarms (tiny noise specs, repetitive leaf motion regions, etc.). This can be done via morphological operations and by checking persistence of motion across a few frames.

Object Detection Stage: Run your YOLO (YOLOv5/7/X) model focused on the ROIs. In practice, you can run YOLO on the full frame (since it’s fast) but then ignore detections that don’t coincide with any motion region – this way you don’t get spurious detections on static objects. For each motion ROI, check if YOLO detects a Human or Car with high confidence inside it. If yes, tag that ROI with the class label (your pipeline now has a confirmed object type). If YOLO doesn’t fire or is low-confidence on that region, treat it as an unknown moving object (the system still tracks it, and you might display it for a human operator to review if needed).

Tracking Stage: Feed the confirmed detections (including “unknown mover” blobs) into a multi-object tracker like DeepSORT or ByteTrack. The tracker will assign IDs and maintain trajectories over time. This stage improves consistency: it links the detections frame-to-frame, smooths out jitter with a Kalman filter, and can re-acquire objects after brief disappearance. Each tracked object can carry metadata like “this track is classified as Human (with some confidence) and has been moving for 10 seconds from left to right”. Tracking gives you velocity (which can help predict where the object is heading) and helps differentiate multiple targets in view.

Decision/Output Stage: Finally, decide what to do with this information. For example, raise an alarm or log an event when a new track appears that is classified as a human or car and persists for a certain number of frames. You might have a rule that unclassified moving blobs also trigger an alert (since they could be very distant humans that the model can’t label). Because the tracker provides a continuous position, you could even estimate the distance or heading of the target if the camera calibration is known.

By following this pipeline, you achieve a balanced system: motion detection ensures you rarely miss a potential target (robust recall for tiny objects), while the YOLO model and tracking provide verification and reduce false positives (maintaining high precision by requiring motion + object recognition). Crucially, this approach does not overly depend on having seen every tiny object in the training data – it leverages fundamental motion as the primary cue for discovery, using learning-based vision only where it’s reliable. This kind of motion-first, recognition-second strategy is common in surveillance systems and is well-suited to thermal imagery where appearance can be ambiguous at range.

In summary, to support your use case: deploy a strong motion detection algorithm at the front of your pipeline, then use your fine-tuned model to classify the moving regions, and maintain those detections with a tracker for stability. This will handle easy cases (where YOLO works outright) and tough cases (where motion is the saving grace) in a unified framework. With careful tuning of the motion detector and integration of tracking, you should significantly improve detection of far-away targets without needing an impractically large specialized training dataset for those scenarios. Good luck with your implementation!

Sources:

Chenming Li et al., “Detection and Tracking of Moving Targets for Thermal Infrared Video Sequences,” Sensors, 2018 – (Discusses the difficulty of thermal IR surveillance, noting low resolution/contrast of distant targets and the use of background subtraction + tracking to handle multiple moving people).

Wei Huang et al., “Moving target detection algorithm based on improved visual background extraction,” Infrared Physics & Technology, 2015 – (Shows that ViBe background subtraction can “quickly, accurately” detect moving targets, and introduces improvements to reduce false alarms over time).

Encord Blog, “Top 10 Video Object Tracking Algorithms in 2026,” 2024 – (Overview of object tracking approaches; notes that deep learning trackers like DeepSORT can add new objects mid-video and keep existing tracks, and that frameworks like YOLOv8 integrate trackers such as ByteTrack for multi-object tracking).

Encord Blog, “YOLO for Object Detection (Guide)” – (Describes YOLO as a single-stage detector that processes images in one pass for real-time object detection). This is the backbone of your Stage 2 classification, fine-tuned to thermal imagery.

Chenming Li et al., Sensors 2018 – (Demonstrates a track-before-detect approach using a labeled RFS filter, achieving better multi-target detection and continuous tracking in thermal videos by combining detection and tracking in one framework). This supports the idea of accumulating evidence across frames for tiny, hard-to-detect objects.