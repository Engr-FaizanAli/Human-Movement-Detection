Motion Detection Pipeline — Development Notes
=============================================
Project: Human-Movement-Detection (TI)
Status:  v2 implemented (stabilisation + PTZ-aware gating)
Updated: 2026-02-25


================================================================================
CONTEXT & MOTIVATION
================================================================================

The YOLO pipeline works well on close-range targets, but at 2-3 km:
  - Human footprint shrinks to 2-8 pixels
  - Thermal contrast drops significantly
  - Appearance-based detection loses reliability

The solution: a MOTION-FIRST pipeline.
  Stage 1  →  Detect any moving blob (class-agnostic) — THIS FILE
  Stage 2  →  YOLO classification on motion ROIs (future)
  Stage 3  →  ByteTrack/DeepSORT tracking (future, already exists in video_inference.py)

This document covers Stage 1 only. Stage 1 is intentionally isolated from YOLO
so it can be tested and tuned on both easy (close-range) and difficult (2-3km)
videos independently.


================================================================================
WHAT WAS BUILT — v1  (ti_motion_detect.py)
================================================================================

Script: scripts/ti_motion_detect.py  (~820 lines)

Architecture: 4 classes + main()

  TIPreprocessor       CLAHE + IIR temporal smoothing
  MotionDetector       MOG2 / KNN / Frame-Diff / Farneback (unified interface)
  MaskPostprocessor    morphology → connected components → temporal persistence
  MotionVisualizer     box drawing, HUD overlay, 3-panel debug panel

Pipeline per frame:
  raw frame
    → TIPreprocessor.process()       (CLAHE + IIR)
    → MotionDetector.apply()         (raw binary mask)
    → [skip if frame_idx < warmup]
    → MaskPostprocessor.apply_morphology()
    → MaskPostprocessor.extract_blobs()
    → MaskPostprocessor.update_tracker()   → (confirmed, candidates)
    → MotionVisualizer.draw_boxes()        (green=confirmed, yellow=candidate)
    → MotionVisualizer.draw_hud()
    → [debug panel if --debug]
    → VideoWriter.write()

Key TI design decisions:
  Problem                              Solution
  ────────────────────────────────     ──────────────────────────────────────────
  Low contrast at 2-3km                CLAHE clip=2.5, tile=12 (local enhancement)
  Per-frame thermal speckle            IIR smoothing alpha=0.4 (~2-frame average)
  TI has no shadow semantics           detectShadows=False on MOG2/KNN
  Single-frame noise bursts            Temporal persistence tracker: N consecutive frames
  Noise streaks vs compact blobs       Solidity filter (streaks ~0.1, humans ~0.4-0.8)
  2x2px blobs invisible to operator    Auto-expand boxes by 4px when area < 16px²
  MOG2 all-foreground during init      Warmup suppression for first history/2 frames

Warmup logic:
  warmup_frames = history // 2
  During warmup: no post-processing, "WARMING UP" HUD overlay displayed.
  Reason: MOG2 produces near-all-foreground mask during first ~20 frames while
  the Gaussian model per pixel is uninitialised.


--- v1 post-session additions (2026-02-25) ---------------------------------------

Two new post-processing parameters added to MaskPostprocessor + CLI:

1. --min-displacement  (default: 0 = disabled)
   Every TrackedBlob now stores its initial_centroid (where it first appeared).
   A blob is only CONFIRMED if:
     frames_seen >= persistence  AND  displacement(current, initial) >= min_displacement

   Why: thermal shimmer / DCT artifacts flicker in-place → displacement near 0.
   A bike/person crossing a road accumulates 40-200px displacement easily.
   This is the single most effective filter for static-camera false positives.

   Recommended: 30-80px for road/perimeter scenes.
   Box label now shows: "#3 T:15 D:62px" — use this to calibrate the value.

2. TrackedBlob.displacement property (computed from initial_centroid to centroid)
   Available on both confirmed and candidate tracks.


--- v1 CLI arguments (full table) -----------------------------------------------

Group              Flag                    Default    Notes
─────────────────  ──────────────────────  ─────────  ──────────────────────────
I/O                --input / -i            required   Input video path
                   --output / -o           auto       <input>_motion.mp4
                   --display               off        Show live OpenCV window
                   --debug                 off        3-panel debug view (3x wide)
                   --scale                 1.0        Resize factor

Detection method   --method                mog2       mog2/knn/diff/farneback
                   --history               200        BG model memory (frames)
                   --threshold             12.0       Sensitivity (lower=more)
                   --diff-frames           5          Frame gap for diff method

TI preprocessing   --clahe / --no-clahe    ON         Toggle CLAHE
                   --clahe-clip            2.5        CLAHE clip limit
                   --clahe-tile            12         CLAHE tile size (px)
                   --temporal-smooth /     ON         Toggle IIR smoothing
                     --no-temporal-smooth
                   --alpha                 0.4        IIR current-frame weight

Post-processing    --morph-close           5          Closing kernel (fill holes)
                   --morph-open            3          Opening kernel (remove noise)
                   --min-area              4          Min blob area px²
                   --max-area              5000       Max blob area px²
                   --min-solidity          0.3        Min blob compactness 0-1
                   --persistence           3          Frames to confirm blob
                   --min-displacement      0.0        Min px travel to confirm
                                                      (0=disabled; 30-80 for roads)


--- ti_motion_tune.py (Streamlit app) -------------------------------------------

Run:
  streamlit run scripts/ti_motion_tune.py
  streamlit run scripts/ti_motion_tune.py --server.port 8502 --server.address 0.0.0.0

Features:
  - Loads video and builds background model on demand
  - 4-panel view: Original | Preprocessed | Raw Motion Mask | Detections
  - Background model params (method, threshold, CLAHE, etc.) require Rebuild click
  - Post-processing params (area, morphology, persistence) update instantly
  - "Raw Mask coverage %" diagnostic — key metric:
      > 30%  →  BG model failing (threshold too low, camera moving, CLAHE+compression)
      10-30% →  noisy, may be tunable
      < 10%  →  healthy, post-processing can isolate targets
  - Export tab: shows exact CLI command to reproduce tuned settings
  - Export full video button: applies current post-processing to cached masks

Key architecture: builds background model ONCE and caches raw masks in
st.session_state, so post-processing params can be tuned instantly without
reprocessing the background model.


================================================================================
WHAT WAS BUILT — v2  (ti_motion_detect_v2.py)
================================================================================

Script: scripts/ti_motion_detect_v2.py  (~600 lines)

Adds two new opt-in stages at the front of the pipeline. All v1 parameters
are preserved. New stages are off by default — add --stabilize / --ptz-aware.

New classes added:

  CameraStabilizer       LK optical flow + RANSAC affine + EMA smoothing
  CameraMotionSensor     RANSAC global motion magnitude + state machine

v2 pipeline per frame:
  raw frame
    → [optional] CameraMotionSensor.update(raw_gray)   ← PTZ state decision
    → [optional] CameraStabilizer.stabilize(frame)     ← shake correction
    → TIPreprocessor.process()
    → MotionDetector.apply(learning_rate=...)           ← lr set by PTZ state
    → [skip if warming_up OR ptz_state != ACTIVE]
    → MaskPostprocessor.apply_morphology()
    → MaskPostprocessor.extract_blobs()
    → MaskPostprocessor.update_tracker()
    → MotionVisualizer.draw_boxes() + draw_hud()
    → VideoWriter.write()


--- CameraStabilizer (--stabilize) ----------------------------------------------

Purpose: corrects handheld/mount vibration before background subtraction.

Algorithm per frame:
  1. goodFeaturesToTrack (Shi-Tomasi)  →  sparse corners in previous gray frame
  2. calcOpticalFlowPyrLK             →  track corners into current frame
  3. estimateAffinePartial2D(RANSAC)  →  4-DOF model (translate + rotate only,
                                         no shear/scale). RANSAC discards moving
                                         object pixels as outliers.
  4. Accumulate raw trajectory [cum_dx, cum_dy, cum_da]
  5. EMA smooth: smooth += alpha * (cum - smooth)  where alpha = 2/(radius+1)
  6. Correction warp = smooth - raw (where we want to be minus where we are)
  7. warpAffine with BORDER_REPLICATE (no black border triangles)

Why RANSAC is key:
  Dense optical flow sees everything — objects AND camera. RANSAC fits only the
  dominant (majority) motion pattern. Moving objects are statistical outliers and
  get discarded automatically. This is also why the same approach works for
  PTZ motion detection (see CameraMotionSensor below).

Tuning --stabilize-radius:
  Large (50-100): very stable output, handles slow drift well, may over-crop at edges
  Small (10-20):  faster response to intentional repositioning, less border crop
  Default 30 is a good starting point for handheld phone footage.


--- CameraMotionSensor (--ptz-aware) --------------------------------------------

Purpose: detects camera pan/tilt and manages background model lifecycle.

Algorithm:
  Same LK + RANSAC affine estimation as CameraStabilizer, but instead of
  correcting the frame, we extract the translation magnitude in px/frame.
  If magnitude > --ptz-motion-thresh: camera is moving.

State machine:
  MOVING   →  pan/tilt detected (motion > threshold px/frame)
               learningRate=0  (BG model FROZEN — not corrupted by pan frames)
               Detections suppressed. HUD shows "PTZ MOVING  12.3 px/f".

  SETTLING →  camera just stopped. Background model being rebuilt from scratch.
               learningRate=0.05  (fast learning, ~20x faster than default)
               Detections suppressed. HUD shows progress bar "SETTLING BG 63%".

  ACTIVE   →  static for --ptz-settle-time seconds. Full detection.
               learningRate=-1  (MOG2/KNN adaptive default)

On MOVING→SETTLING transition:
  detector.reset() + postproc.reset() + preprocessor.reset() are all called.
  The background model is trained ONLY on static footage. This eliminates the
  classic pan artifact where MOG2 learns pan blur as "background" for seconds.

NOTE: CameraMotionSensor runs on the raw (pre-stabilisation) frame. The stabilizer
corrects small shake; the motion sensor detects large intentional pans. Keeping
them separate means one doesn't interfere with the other.

Tuning --ptz-motion-thresh:
  The HUD shows live motion in px/frame. Run with --ptz-aware on a test clip,
  read the MOVING value during an actual pan (e.g. 15-40px/f typically) and the
  static value (usually 0.5-2px/f). Set threshold to midpoint.
  Default 5.0 is reasonable for most PTZ cams; try 3-4 for sensitive detection.


--- On not having hardware sensor values ----------------------------------------

Context: PTZ unit has no IMU/encoder output available (no SDK access).
Question: Is CV-only PTZ detection sufficient?

Short answer: Yes, for this use case.

Long answer by scenario:

  Scenario 1 — Large manual pan (30°+ repositioning every 30 min):
    RANSAC detects 15-50px/frame of global motion easily.
    State machine fires reliably. This is the primary use case. WORKS WELL.

  Scenario 2 — Micro-vibration during "static" hold (e.g. wind, motor hum):
    CameraStabilizer handles this before it reaches the BG subtractor.
    Vibration is small (1-3px), does not trigger MOVING state.
    WORKS WELL.

  Scenario 3 — Very slow creep/drift (1-2px/frame):
    Hard to threshold. BUT slow drift also doesn't corrupt MOG2 badly —
    the model adapts over its history window. ACCEPTABLE.

  Scenario 4 — Pan during an active detection:
    Detection is lost during pan + settle window (5-10s).
    THIS IS THE UNAVOIDABLE COST without hardware IMU.
    Military systems use IMU to maintain detection during pan via homography
    compensation against a panoramic background model. Complex to implement.
    For surveillance with ~30 min repositioning intervals: ACCEPTABLE.

What military-grade systems actually do:
  1. Hardware encoder/IMU on the PTZ unit → pan/tilt angles fed directly.
     No CV estimation needed. Immediate, zero-latency state change.
  2. Panoramic background model: a full spherical background image is maintained.
     Each frame is registered to the panorama (not to a reference frame), so
     background subtraction works even DURING a pan.
  3. Multi-camera overlapping coverage: at least one camera is always static.
  The CV-only approach implemented here is the pragmatic software equivalent
  that works well when the camera is mostly static.


--- v2 CLI arguments (new flags only, all v1 flags also present) -----------------

Group              Flag                    Default    Notes
─────────────────  ──────────────────────  ─────────  ──────────────────────────
Stabilisation      --stabilize             off        Enable shake stabilisation
                   --stabilize-radius      30         EMA window (frames)

PTZ-aware          --ptz-aware             off        Enable PTZ state machine
                   --ptz-motion-thresh     5.0        px/frame to classify MOVING
                   --ptz-settle-time       5.0        Seconds static before ACTIVE


--- v2 recommended commands (PTZ static cam, bike on road) ----------------------

# MOG2 with stabilisation + displacement filter
python scripts/ti_motion_detect_v2.py \
  --input test_cases/test_10_rev.mp4 \
  --stabilize --no-clahe \
  --threshold 30 --min-area 50 \
  --persistence 12 --min-displacement 40

# KNN + PTZ-aware (camera repositioned every 30 min)
python scripts/ti_motion_detect_v2.py \
  --input test_cases/test_10_rev.mp4 \
  --method knn --ptz-aware \
  --no-clahe --history 400 --threshold 20 \
  --min-area 50 --persistence 12 --min-displacement 40 \
  --ptz-motion-thresh 5.0 --ptz-settle-time 5.0

# Both stabiliser + PTZ-aware + debug view (use for initial calibration)
python scripts/ti_motion_detect_v2.py \
  --input test_cases/test_10_rev.mp4 \
  --stabilize --ptz-aware --debug \
  --no-clahe --threshold 30 --min-area 50

# diff method (no warmup period, no BG model to corrupt)
python scripts/ti_motion_detect_v2.py \
  --input test_cases/test_10_rev.mp4 \
  --method diff --stabilize \
  --no-clahe --diff-frames 8 --threshold 25 \
  --min-area 50 --persistence 10 --min-displacement 40


================================================================================
TUNING LESSONS LEARNED
================================================================================

1. CLAHE + compressed video = false positives everywhere
   CLAHE applies per-tile histogram equalization. When a moving person enters a
   tile, the tile histogram shifts, causing static background pixels in that tile
   to be remapped differently. MOG2 flags these as foreground.
   FIX: --no-clahe for any compressed (H.264/MP4) video.
        --clahe-tile 32 or 64 (larger tiles) reduces this for TI footage.
        Only enable CLAHE for native (uncompressed) TI sensor output.

2. IIR temporal smooth can hurt on regular video
   IIR smoothing creates temporal correlations that interact poorly with MOG2's
   independence assumption. For regular video, disable it.
   FIX: --no-temporal-smooth unless you have genuine thermal speckle noise.

3. min-area=4 is ONLY for extreme-range (3km) TI
   At normal/close range a person is 500-5000+ px². min-area=4 passes every
   compression artifact and sensor hot pixel as a candidate.
   FIX: Set --min-area based on expected target size for your range:
        1km TI: ~150-600 px²    2-3km TI: 4-30 px²    Regular video: 500-5000 px²

4. persistence=3 is met by H.264 block artifacts
   DCT block artifacts are spatially consistent frame-to-frame and persist 3+
   frames easily. For compressed video, raise to 8-15.
   FIX: --persistence 12-15 for compressed video.

5. Camera movement kills background subtraction
   All test videos had camera movement. MOG2/KNN assume a static background.
   Even tiny vibration causes the entire background to be flagged as foreground.
   FIX: --stabilize for shake/vibration; --ptz-aware for pan/tilt. See v2.

6. min-displacement is the best single FP reducer for static-cam scenes
   Shimmer, DCT blocks, sensor noise — they all appear in place. Anything that
   doesn't travel 30+ pixels from its first detection is not a real target.
   FIX: --min-displacement 40 as a baseline for road/perimeter scenes.
        Watch the "D:XXpx" label on confirmed boxes to calibrate your scene.

7. The diff method is the most robust to all the above problems
   No background model = no warmup, no corruption from camera motion, no slow
   adaptation. The cost: no persistence of the model across time, so slow targets
   need large --diff-frames (8-15).
   FIX: Start with --method diff --diff-frames 8 when everything else fails.

8. MOG2/KNN background absorption — objects detected partially then lost
   MOG2/KNN are CONTINUOUSLY adaptive. They never stop learning. After ~100-200
   frames of a moving object occupying the same pixels, the model absorbs it as
   background. Detection weakens progressively → track resets → eventually nothing.
   This is NOT a warmup issue — it happens throughout the entire video.
   Sun/shadows do NOT affect TI cameras; only slow thermal drift (hours scale) does.

9. --freeze-after-warmup sounds like the fix but trades one problem for another
   Freezing the model (learningRate=0 after warmup) stops background absorption of
   the target, but also stops the model from absorbing persistent noise and thermal
   sensor flicker that the adaptive model would quietly clean up. Net result on TI
   compressed video: more persistent FPs, comparable or slightly worse overall.
   VERDICT: --freeze-after-warmup is not reliable for TI. Use --method diff for
   long-journey detection.


--- Commands that worked on specific test cases ---------------------------------

test_4_static.mp4 (partially static drone crop):
  python scripts/ti_motion_detect.py \
    --input test_cases/test_4_static.mp4 \
    --method diff --history 200 --threshold 30 --diff-frames 10 \
    --no-clahe --no-temporal-smooth \
    --morph-close 3 --morph-open 1 \
    --min-area 1 --max-area 100 --min-solidity 0.3 --persistence 15

test_10_rev.mp4 (static PTZ, bike at 4-5s):
  python scripts/ti_motion_detect_v2.py \
    --input test_cases/test_10_rev.mp4 \
    --stabilize --ptz-aware --no-clahe \
    --threshold 30 --min-area 50 \
    --persistence 12 --min-displacement 40

test_18.mp4 (raw NVR TI, static camera — confirmed good results):
  Context: Direct NVR output, no H.264 recompression. CLAHE and temporal-smooth
  are NOW safe and beneficial. Threshold can go as low as 8-10. min-area 10-20.

  Baseline (MOG2, no preprocessing):
  python motion_scripts/ti_motion_detect_v2.py --input test_cases/test_18.mp4 --no-clahe --no-temporal-smooth --threshold 10 --min-area 20 --max-area 50000 --min-density 0.15 --persistence 4 --min-displacement 10 --spatial-tol 20 --max-absent 4 --no-candidates --output test_18_static.mp4

  MOG2 + CLAHE (local contrast enhancement for faint distant targets):
  python motion_scripts/ti_motion_detect_v2.py --input test_cases/test_18.mp4 --clahe --clahe-clip 2.5 --clahe-tile 12 --no-temporal-smooth --threshold 8 --min-area 20 --max-area 50000 --min-density 0.15 --persistence 4 --min-displacement 10 --spatial-tol 20 --max-absent 4 --no-candidates --output test_18_clahe.mp4

  MOG2 + full TI preprocessing (CLAHE + temporal smooth for speckle reduction):
  python motion_scripts/ti_motion_detect_v2.py --input test_cases/test_18.mp4 --clahe --clahe-clip 2.5 --temporal-smooth --alpha 0.35 --threshold 8 --min-area 10 --max-area 50000 --min-density 0.15 --persistence 5 --min-displacement 10 --spatial-tol 20 --max-absent 5 --no-candidates --output test_18_fullpreproc.mp4

  KNN + full TI preprocessing (more accurate non-parametric model):
  python motion_scripts/ti_motion_detect_v2.py --input test_cases/test_18.mp4 --method knn --history 300 --clahe --clahe-clip 2.5 --temporal-smooth --alpha 0.35 --threshold 12 --min-area 10 --max-area 50000 --min-density 0.15 --persistence 5 --min-displacement 10 --spatial-tol 20 --max-absent 5 --no-candidates --output test_18_knn.mp4

  MOG2 frozen after warmup (viable on raw TI — no H.264 noise to expose):
  python motion_scripts/ti_motion_detect_v2.py --input test_cases/test_18.mp4 --clahe --clahe-clip 2.5 --temporal-smooth --alpha 0.35 --threshold 8 --freeze-after-warmup --min-area 10 --max-area 50000 --min-density 0.15 --persistence 5 --min-displacement 10 --spatial-tol 20 --max-absent 5 --no-candidates --output test_18_frozen.mp4

  diff method (no absorption, full journey detection):
  python motion_scripts/ti_motion_detect_v2.py --input test_cases/test_18.mp4 --method diff --diff-frames 6 --clahe --clahe-clip 2.5 --temporal-smooth --alpha 0.35 --threshold 12 --min-area 10 --max-area 50000 --min-density 0.15 --persistence 4 --min-displacement 10 --spatial-tol 20 --max-absent 5 --no-candidates --output test_18_diff.mp4


================================================================================
FILE INVENTORY
================================================================================

scripts/ti_motion_detect.py     Stage 1 motion detection CLI v1 (~820 lines)
scripts/ti_motion_detect_v2.py  Stage 1 v2: + CameraStabilizer + PTZ gating
scripts/ti_motion_tune.py       Streamlit interactive tuning app
scripts/video_inference.py      Stage 2+3 YOLO + ByteTrack (existing, unmodified)
scripts/sahi_video_inference.py Stage 2+3 with SAHI slicing (existing, unmodified)

docs/motion_detection.txt           Original research doc (three-stage pipeline)
docs/claude_motion_detection_v1.txt THIS FILE — implementation notes


================================================================================
ROADMAP
================================================================================

[v1 - DONE]
  - ti_motion_detect.py: MOG2/KNN/diff/farneback with full parameter control
  - ti_motion_tune.py: Streamlit tuner with 4-panel view
  - Identified camera motion as core problem
  - Added --min-displacement: displacement-based confirmation filter

[v2 - DONE]
  - CameraStabilizer: LK optical flow + RANSAC + EMA trajectory smoothing
  - CameraMotionSensor: RANSAC global motion → MOVING/SETTLING/ACTIVE state machine
  - PTZ-aware background gating: freeze/rebuild BG model based on camera state
  - MOG2/KNN learningRate scheduling per state (0 / 0.05 / -1)
  - MotionDetector.reset() for clean model rebuild after pan
  - PTZ state overlay in HUD with progress bar during SETTLING

[v3 - DONE]
  - EMA bbox smoothing per track (--bbox-smooth-alpha): damps frame-to-frame
    blob shape changes so the displayed box moves smoothly
  - Velocity-predicted track matching (--vel-smooth-alpha): matches blobs to
    predicted centroid position instead of last known — keeps track ID stable
    even when thermal blob shape changes dramatically between frames
  - Label now shows V:px/f (velocity magnitude) for calibration

[v4 - DONE]
  - ExclusionZoneManager: interactive polygon editor, JSON persistence
  - Zones applied after morphology, before blob extraction
  - --draw-zones opens editor; --zone-overlay burns zones onto output video
  - Fixes wind / vegetation false positives permanently per camera position


--- v4 commands ---

# Step 1: draw exclusion zones (one-time per camera position)
python motion_scripts/ti_motion_detect_v4.py \
  --input test_cases/test_18.mp4 --draw-zones
# Opens editor on first frame. Click to place vertices, C to close polygon,
# S to save. Zones saved to test_cases/test_18_zones.json. Script exits.

# Step 2: run detection — zones auto-load from test_18_zones.json
# MOG2 (default)
python motion_scripts/ti_motion_detect_v4.py \
  --input test_cases/test_18.mp4 \
  --no-clahe --threshold 12 --min-area 20 \
  --persistence 4 --min-displacement 10 \
  --spatial-tol 20 --max-absent 5 --no-candidates

# KNN
python motion_scripts/ti_motion_detect_v4.py \
  --input test_cases/test_18.mp4 --method knn \
  --no-clahe --threshold 12 --min-area 20 \
  --persistence 4 --min-displacement 10 \
  --spatial-tol 20 --max-absent 5 --no-candidates

# diff (recommended — no warmup, no BG absorption)
python motion_scripts/ti_motion_detect_v4.py \
  --input test_cases/test_18.mp4 --method diff --diff-frames 6 \
  --no-clahe --threshold 12 --min-area 20 \
  --persistence 4 --min-displacement 10 \
  --spatial-tol 20 --max-absent 5 --no-candidates

# farneback
python motion_scripts/ti_motion_detect_v4.py \
  --input test_cases/test_18.mp4 --method farneback \
  --no-clahe --threshold 12 --min-area 20 \
  --persistence 4 --min-displacement 10 \
  --spatial-tol 20 --max-absent 5 --no-candidates


[v5 - FUTURE: YOLO integration]
  - Integrate Stage 1 output (motion ROIs) as attention mask for YOLO
  - Feed confirmed blobs as ROI crops to YOLO inference
  - Connect to ByteTrack for multi-frame track IDs across stages

[v6 - FUTURE: robustness]
  - Multi-scale processing (different min/max-area pass at different scales)
  - Adaptive threshold based on scene statistics per frame
  - Panoramic background model (enables detection during PTZ pan — eliminates
    the detection gap in MOVING/SETTLING states)
